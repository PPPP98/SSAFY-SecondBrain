# TIL - 웹 크롤링을 통한 데이터 수집

**작성일**: 2025년 10월 15일
**주제**: 웹 크롤링(Web Crawling) 기초 개념 및 데이터 수집 방법

---

## 학습 개요

오늘은 주제를 탐구하면서 데이터 수집을 위한 웹 크롤링을 통해 데이터를 수집하는 방법에 대해 공부했다. 웹상의 방대한 정보를 자동으로 수집하고 분석하는 기술의 기초부터 실제 구현 방법, 그리고 법적·윤리적 고려사항까지 학습했다.

---

## 웹 크롤링이란?

### 기본 개념

**웹 크롤링(Web Crawling)**은 특정 웹사이트에서 원하는 정보를 자동으로 수집할 수 있는 기술이다. 웹 크롤러는 조직적이고 자동화된 방법으로 월드 와이드 웹을 탐색하는 컴퓨터 프로그램이다.

### 크롤링 vs 스크래핑

많은 사람들이 혼동하는 두 개념의 차이점:

- **크롤링(Crawling)**: "어디에 무엇이 있는가"를 찾는 과정
  - 여러 웹 페이지를 기계적으로 탐색하는 방법
  - 검색 엔진이 웹 페이지를 인덱싱하는 방식

- **스크래핑(Scraping)**: "그 중 무엇을 수집할 것인가"에 집중
  - 특정한 하나의 웹 페이지를 탐색하여 원하는 정보만 콕 집어내는 방법
  - 실제 데이터 추출 작업

**결론**: 크롤링으로 페이지를 찾고, 스크래핑으로 데이터를 추출한다!

---

## 주요 활용 분야

웹 크롤링은 다양한 분야에서 활용된다:

1. **검색 엔진**: Google, Naver 등의 웹 페이지 인덱싱
2. **SEO 분석**: 웹사이트 최적화를 위한 데이터 수집
3. **데이터 분석**: 시장 조사, 경쟁사 분석
4. **AI 학습 데이터**: 머신러닝 모델 훈련용 데이터 구축
5. **가격 모니터링**: 전자상거래 가격 비교
6. **뉴스 수집**: 뉴스 애그리게이터 서비스

---

## Python 크롤링 도구

### 1. BeautifulSoup

**특징**:
- HTML/XML 파싱에 특화된 라이브러리
- 정적 웹 페이지 크롤링에 적합
- 간단하고 직관적인 사용법

**장점**:
- 가벼운 라이브러리로 빠른 처리 속도
- CSS 선택자를 사용한 쉬운 요소 추출
- Requests와 함께 사용하여 간단한 크롤링 구현

**단점**:
- JavaScript로 동적 렌더링되는 콘텐츠 처리 불가
- 사용자 인터랙션 시뮬레이션 불가

**기본 사용 예제**:
```python
import requests
from bs4 import BeautifulSoup

# 웹 페이지 가져오기
url = 'https://example.com'
response = requests.get(url)

# HTML 파싱
soup = BeautifulSoup(response.text, 'html.parser')

# 데이터 추출
titles = soup.select('.title')  # CSS 선택자 사용
for title in titles:
    print(title.text)
```

### 2. Selenium

**특징**:
- 웹 브라우저 자동화 도구
- JavaScript 기반 렌더링까지 지원
- 동적 웹사이트 대응에 강력함

**장점**:
- 실제 브라우저를 조작하여 사용자 행동 시뮬레이션 가능
- JavaScript로 생성된 콘텐츠도 크롤링 가능
- 클릭, 스크롤, 폼 입력 등 다양한 인터랙션 가능
- 직관적인 디버깅 방법 (브라우저에서 직접 확인)

**단점**:
- BeautifulSoup보다 느린 처리 속도
- 브라우저 드라이버 설치 필요
- 더 많은 시스템 리소스 사용

**기본 사용 예제**:
```python
from selenium import webdriver
from selenium.webdriver.common.by import By

# 브라우저 드라이버 초기화
driver = webdriver.Chrome()

# 웹 페이지 열기
driver.get('https://example.com')

# 요소 찾기 및 클릭
button = driver.find_element(By.CLASS_NAME, 'submit-button')
button.click()

# 데이터 추출
elements = driver.find_elements(By.CSS_SELECTOR, '.item')
for element in elements:
    print(element.text)

# 브라우저 종료
driver.quit()
```

### 3. BeautifulSoup + Selenium 조합

두 도구를 함께 사용하면 각각의 장점을 활용할 수 있다:

```python
from selenium import webdriver
from bs4 import BeautifulSoup

# Selenium으로 페이지 로드 및 동적 콘텐츠 생성
driver = webdriver.Chrome()
driver.get('https://example.com')

# 페이지 끝까지 스크롤 (동적 로딩 콘텐츠 대응)
driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

# BeautifulSoup로 HTML 파싱
soup = BeautifulSoup(driver.page_source, 'html.parser')

# 데이터 추출
data = soup.select('.content')

driver.quit()
```

---

## 법적·윤리적 고려사항

### robots.txt란?

**robots.txt**는 웹사이트의 루트 디렉토리에 위치한 텍스트 파일로, 웹 크롤러에게 어떤 페이지를 크롤링해도 되는지 알려주는 가이드라인이다.

**위치**: `https://example.com/robots.txt`

**예시**:
```
User-agent: *
Disallow: /admin/
Disallow: /private/
Allow: /public/
```

### robots.txt의 법적 효력

**중요한 사실**:
- robots.txt는 **권고안**이지 법적 강제력은 없다
- 지킬 의무는 없지만, 지켜주는 것이 상식이자 윤리
- 규칙을 지키지 않으면 서버 관리자가 IP를 차단할 수 있음

### 2025년 최신 사례: 퍼플렉시티(Perplexity) 논란

2025년 8월, AI 검색엔진 '퍼플렉시티'가 논란이 되었다:

**문제점**:
- robots.txt 지침을 무시하고 콘텐츠 무단 수집
- 여러 IP 주소를 바꿔가며 일반 사용자처럼 위장 (스텔스 크롤링)
- 웹사이트의 보안 조치를 우회하는 악의적 행위

**교훈**:
- AI 시대에도 윤리적 데이터 수집은 필수
- 기술적으로 가능하다고 해서 모두 허용되는 것은 아님
- 법적 공방으로 이어질 수 있는 민감한 문제

### 크롤링 시 지켜야 할 원칙

1. **robots.txt 확인**: 크롤링 전 반드시 확인하고 존중할 것
2. **이용 약관 준수**: 웹사이트의 이용 약관을 꼭 확인
3. **적절한 요청 간격**: 서버에 부담을 주지 않도록 요청 속도 조절
4. **User-Agent 명시**: 크롤러의 신원을 명확히 밝힐 것
5. **개인정보 보호**: 수집한 데이터의 개인정보 처리에 주의
6. **저작권 존중**: 수집한 데이터의 사용 목적과 범위를 명확히

**올바른 크롤링 예제**:
```python
import requests
import time

headers = {
    'User-Agent': 'MyBot/1.0 (myemail@example.com)'  # 신원 명시
}

# robots.txt 확인
robots_url = 'https://example.com/robots.txt'
robots_response = requests.get(robots_url)
print(robots_response.text)

# 적절한 간격으로 요청
for url in urls:
    response = requests.get(url, headers=headers)
    # 데이터 처리
    time.sleep(2)  # 2초 대기 (서버 부담 최소화)
```

---

## 주요 학습 포인트

### 오늘 배운 핵심 내용

1. **크롤링과 스크래핑의 차이**: 크롤링은 탐색, 스크래핑은 추출
2. **도구 선택 기준**:
   - 정적 페이지 → BeautifulSoup + Requests
   - 동적 페이지 → Selenium
   - 복합적 요구 → 두 도구 조합
3. **법적·윤리적 책임**: 기술적 가능성보다 윤리적 책임이 우선

### 실전 적용 팁

1. **시작 전 체크리스트**:
   - [ ] robots.txt 확인
   - [ ] 이용 약관 검토
   - [ ] 데이터 사용 목적 명확화
   - [ ] 적절한 도구 선택

2. **효율적인 크롤링**:
   - 필요한 데이터만 선별적으로 수집
   - 요청 간격을 두어 서버 부담 최소화
   - 에러 핸들링 구현 (네트워크 오류, 타임아웃 등)
   - 수집한 데이터를 체계적으로 저장

3. **디버깅 방법**:
   - 브라우저 개발자 도구로 HTML 구조 분석
   - 작은 범위부터 테스트 후 확장
   - 로그를 남겨 문제 추적


---

## 참고 자료

- [크롤링 가이드 - TBWA 데이터랩](https://seo.tbwakorea.com/blog/crawling/)
- [웹 크롤링과 스크래핑 - ITPE](https://itpe.jackerlab.com/entry/웹-크롤링Web-Crawling-스크래핑Web-Scraping)
- [Python Selenium 사용법](https://greeksharifa.github.io/references/2020/10/30/python-selenium-usage/)
- [robots.txt 가이드 - Google Developers](https://developers.google.com/search/docs/crawling-indexing/robots/intro)
- [퍼플렉시티 논란 - 바이라인네트워크](https://byline.network/2025/08/0805/)

---

## 회고

웹 크롤링은 단순히 데이터를 수집하는 기술이 아니라, 윤리적 책임과 법적 고려사항이 함께 따라오는 분야임을 깨달았다. 특히 2025년 퍼플렉시티 사례를 통해 AI 시대에도 데이터 수집의 윤리가 얼마나 중요한지 알게 되었다.

기술적으로는 BeautifulSoup과 Selenium의 차이를 명확히 이해하게 되었고, 각 도구의 장단점을 파악하여 상황에 맞게 선택할 수 있게 되었다. 다음 단계로는 실제 프로젝트를 진행하며 실전 경험을 쌓아야겠다.

**핵심 교훈**: "할 수 있다고 해서 다 해도 되는 것은 아니다. 기술력만큼 윤리의식도 중요하다."
