# TIL

## n8n 코드리뷰 자동화 보완

- 팀원들의 개발 속도 증진을 위해 MCP 서버 운용 계획중

### 현재 구조

1. GitLab에서 MR 발생 시 Webhook 발생 -> E107 워크플로우 Trigger
   - MR open, 댓글에 '리뷰해주세요' 입력 시에만
   - 빠른 리뷰는 QuickEye 호출, 상세 리뷰는 DeepDive 호출
2. HTTP Request 노드로 GMS의 Anthropic(claude-sonet-4) 요청
3. 응답 파싱하여서 GitLab MR의 댓글에 코드 리뷰 생성

### Context 7 MCP

- MCP는 LLM과 도구를 표준화함. LLM이 MCP에 어떤 툴이 있는지 알려달라고 하면 툴을 실행하고, 결과까지 전송해줌.
- Context7 MCP는 코드 공식 문서를 베이스로 LLM이 끌어다 쓸 수 있게 해줌.
- 여기에 활용할 OpenRouter는 여러 모델을 OpenAI 스타일로 묶어주는 모델 게이트웨이임.

### 계획

1. Context7 MCP 서버 띄우기
   - n8n과 같은 EC2에 컨테이너나 프로세스로 설치를 해야 함
2. OpenRouter MCP Bridge
   - `https://openrouter.ai/api/v1/chat/completions` 같은 url에 tools가 MCP에서 변환된 형태가 바디에 포함되어서 보내짐
3. n8n 워크플로우에 현재 Claude via GMS라는 HTTP 노드 옆에 OpenRouter via MCP 노드를 추가
   - Build Claude Prompt 노드에서는 MR diff, 파일명, 변경점을 Output으로 내보냄.
   - 이 Output을 조건 분기를 걸거나 Claude via GMS, OpenRouter via MCP 노드 두 군데 모두 연결.
4. MR에 연속 댓글을 달거나, Context7 활용 답만 댓글에 달기.